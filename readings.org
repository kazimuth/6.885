#+TITLE: extra reading notes
#+AUTHOR: james gilles
#+EMAIL: jhgilles@mit.edu
#+DATE: 04 february 2020
#+OPTIONS: tex:t latex:t
#+STARTUP: latexpreview

#+LATEX_HEADER: \newcommand{\prob}[0]{\mathrm{p}}}
#+LATEX_HEADER: \newcommand{\df}[1]{\mathrm{d} #1}

http://probcomp.csail.mit.edu/reading-list/

* When are probabilistic programs probably computationally tractable?
  http://danroy.org/papers/FreerManRoy-NIPSMC-2010.pdf

  Two core questions:

  a. What conditions suffice for Bayesian inference by posterior simulation to be computationally efficient?
  b. How should probabilistic programmers write their probabilistic programs so as to maximize their probable tractability?

  Three notes:

  1. Surprise upper-bounds the complexity of exact posterior simulation. In fact, the surprise in thedata can sometimes provide
     a sharper characterization of the computational tractability of posterior simulation than measures based on, e.g.,
     the dimensionality, tree-width of an underlying graphical model, etc.

  2. Approximate posterior simulation is sometimes quite tractable, especially when either the surprise can be broken into
     small steps, or the constraints (induced by the data) on the program’s random choices are sparsely overlapping.

  3. Probabilistic programmers can increase the likely tractability of their programs, without resorting
     to tempering methods, by designing them to be self-relaxing.

** 1: surprise upper-bounds complexity of simulation
   > We have two constructive results showing that exact posterior simulation via rejection is tractable when the data are not too surprising, or when the surprise can be broken down into a short enough sequence of sufficiently unsurprising steps. These results provide a conservative estimate of the difficulty of practical probabilistic programming.

   Also more sharply bound things than classical "syntactic" results.

   > Importantly, our results do not underestimate the anticipated complexity of problemsbelieved to be truly hard (e.g., inverting cryptosystems).

   ...some analytic analysis of rejection sampling...

   > Sloganized, then, this result says that concept classes that admit tight generalization bounds also admit tractable rejection samplers, and vice versa.

** 2: approximate posterior simulation is sometimes quite tractable
   e.g. MCMC

** 3: probabilistic programmers can increase the likely tractability of their programs by designing them to be self-relaxing.
   > The key idea takes inspiration from an aspect of evolution: complex structures often arise in part because there is a smooth bridge along which fitness mostly increases, running through intermediate structures. The very high dimensional nature of the space of possible structures provides the degrees of freedom that enable these smooth paths to exist. Annealing-type methods attempt to introduce this kind of structure in a post-hoc fashion, by adding variables like temperature (or byco-opting variables like various weighting parameters) and smoothly changing them in ways that facilitate mixing.

   > Our approach is motivated by a different intuition: one driver of slow convergence (supported by both our experience and the sufficient conditions we previously mentioned) is strong global dependence. When large groups of variables are constrained in a near-deterministic fashion, rejection often becomes unlikely to succeed, and generic inference techniques based on modifying a small number of variables one at a time often converge slowly.

   ...so instead, you add *more* global parameters. This seems to make the problem *harder* in a
   traditional context, but in practice it works better. (c.f. line estimations from lec 1.)





* Bayesianism and Causality, or, why I am only a half-bayesian
  http://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf

  core assertion: statistics cannot describe all knowledge, because it cannot describe *causality*.

  roughly: statistics is for non-experimental data, and causality is for experimental data?

  what is meant exactly by "causality" in this setting?
  > Examples of statistical concepts are: correlation, regression,dependence, conditional independence, association, likelihood, collapsibility, riskratio, odd ratio, and so on. Examples of causal concepts are: randomization, in-fluence, effect, confounding, disturbance, spurious correlation, instrumental vari-ables, intervention, explanation, attribution, and so on.

  causality must be defined through causality

  causality is easy to understand, and statistics is hard, which makes it easier to argue about

  causality is not necessarily falsifiable?


  example notation: $X \to Y$, "$X$ causes $Y$", i.e. Y depends on X but X does not depend on Y.
  chain model: $X \to Y \to Z$: encodes seven causal assumptions, represented by presence or lack of a
  directed edge.

  do-calculus: take $Z \to X \to Y$
  $z = f_Z(w)$
  $x = f_X(z, v)$
  $y = f_Y(x, u)$

  $w \sim W, v \sim V, u \sim U$: independent, unknown distribution

  operation $do(x)$ simulates *physical intervention* by replacing certain functions with constants.

  e.g. $do(X = x_0)$ results in:
  $z = f_Z(w)$
  $x = x_0$
  $y = f_Y(x, u)$

  new distribution: $P(y, z | do(x_0))$

  > Probability theory deals with beliefs about an uncertain, yet static world, while causality deals with changes that occur in the world itself, (or in one’s theory of such changes). More specifically, causality deals with how probability functions change in response to influences (e.g., new conditions or interventions) that originate from outside the probability space, while probability theory, even when given a fully specified joint density function on all (temporally-indexed) variables in the space, cannot tell us how that function would change under such external influences. Thus, “doing” is not reducible to “seeing”, and there is no point trying to fusethe two together.

  > Many philosophers have aspired to show that the calculus of probabilities, en-dowed with a time dynamic, would be sufficient for causation [Suppes, 1970]. Awell known demonstration of the impossibility of such reduction (following Otte[1981]) goes as follows. Consider a switch X that turns on two lights, Y and Z,and assume that, due to differences in location, Z turns on a split second before Y. Consider now a variant of this example where the switch X activates Z, and Z, in turns, activates Y. This case is probabilistically identical to the previous one, because all functional and temporal relationships are identical. Yet few people would perceive the causal relationships to be the same in the two situations; the latter represents cascaded process, $X \to Z \to Y$, while the former representsa branching process, $Y \leftarrow X \to Z$. The difference shows, of course, when we consider interventions; intervening on Z would affect Y in the cascaded case, but not in the branching case.

  > A set of mechanisms, each represented by an equation, is not equivalent to the set of algebraic equations that are implied by those mechanisms. Mathematically, the latter is defined as one set of n equations, whereas the former is defined as n separate sets, each containing one equation. These are two distinct mathematical objects that admit two distinct types of solution-preserving operations.

  > Indeed, no mathematical machinery can ever verify whether a given DAG really represents the causal mechanisms that generate the data — such verification is left either to human judgment or to experimental studies that invoke interventions. I submit, however, that neither suspicion nor mistrust are justified in the case at hand; DAGs are no less formal than mathematical equations, and questions of model verification need be kept apart from those of conceptual definition.

  > Remarkably, by taking seriously the abstract (and untestable) notion of a distribution, we obtain a license to ignore it. An analogous logic applies to causation.

  > The separation between concept definition and model verification is even more pronounced in the Bayesian framework, where purely judgmental concepts, such as the prior distribution of the mean, are perfectly acceptable, as long as they can be assessed reliably from one’s experience or knowledge.



* Why I am not a Bayesian

  Confirmation theories (i.e. theories of inductive logic): currently mostly focus on probability.
  However, this hasn't generally been useful in the history of science.

  > Probability is a distinctly minor note in the history of scientific argument.

  So is science just a bunch of random guesses then?...

  Bayesianism asserts that we have "degrees of belief", and these degrees of belief can be modeled as probabilities, and measured
  by having people place bets and assuming that they will only take bets whose expected values are positive.

  - degrees of belief: yeah sure, in some cases
  - modeled as probabilities: hmmm
  - ...that can be measured placing bets: lol no

  bayesianism provides no link to reality. however, result: all bayesians will converge to the truth.
  ...but, reviewed: actually only shows that *bayesians believe bayesians will converge to the truth*.

  so why do people like bayesianism? not convinced by technical arguments; instead, because bayesianism produces some results
  which seem to coincide with common sense. also, bayesianism is flexible enough to provide explications of many "subtleties and vagaries"
  of scientific reasoning.

  but, clapback: sure, you can pick your priors "more or less ad hoc", and get whatever "particular *inferences*" you want out, but we learn nothing
  from this line of argument.

  > What we want is an explanation of *scientific argument*; what the Bayesians give us is a theory of learning, indeed a theory of *personal learning*.

  A good question: why is *argumentation* useful?
  ...other examples of aspects of the scientific method, which Bayesianism can cover more or less well, or not at all...

  What about simplicity?
  Harold Jeffreys tried to enumerate hypotheses such that simpler ones had higher prior probabilities than more complex ones. unfortunately his scheme
  allowed one to immediately deduce that all hypotheses have probability 0... more broadly, there's no way

  new theories that explain evidence from the past don't make sense in the context of ideal bayesians, who would never be surprised by a new theory...
  so if bayesianism is supposed to explain the real behavior of scientists, it fails here.



* what is...
** rejection sampling?
   https://en.wikipedia.org/wiki/Rejection_sampling

   ...this is the dumb algorithm from statmech...

** markov chain monte carlo?
   https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo

   In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis–Hastings algorithm.

   ...this is the smart algorithm from statmech...

** a tempering method?
** confirmation theory?
   https://plato.stanford.edu/entries/confirmation/

   > In contemporary philosophy, confirmation theory can be roughly described as the area where efforts have been made to take up the challenge of defining plausible models of non-deductive reasoning. Its central technical term -- confirmation -- has often been used more or less interchangeably with “evidential support”, “inductive strength”, and the like.

* MCMC reading
** inference
  https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29

  $$\prob(\theta | x) = \frac{\prob(x | \theta) \prob(\theta)}{\prob(x)}$$

  $\prob(\theta)$ : prior
  $\prob(x | \theta)$ : likelihood

  $$\prob(x) = \int_\theta \prob(x | \theta) \prob(\theta) \; \df{\theta}$$ : evidence

** monte carlo
  https://en.wikipedia.org/wiki/Monte_Carlo_method

  simulations using random sampling to compute something.

  > There is no consensus on how Monte Carlo should be defined. For example, Ripley[49] defines most probabilistic modeling as stochastic simulation, with Monte Carlo being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky[50] distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior). Examples:

** monte carlo integration
  https://en.wikipedia.org/wiki/Monte_Carlo_integration

  problem: compute multidimensional definite integral:

  $$I=\int _{{\Omega }}f(\overline {{\mathbf {x}}})\,d\overline {{\mathbf {x}}}$$

  where $\Omega$, a subset of $\mathbb{R}^m$, has volume

  $$V=\int _{\Omega }d{\overline {\mathbf {x} }}$$

  Then:

  $$I\approx Q_{N}\equiv V{\frac {1}{N}}\sum _{i=1}^{N}f({\overline {\mathbf {x} }}_{i})=V\langle f\rangle $$

  and $\lim_{N \to \infty} Q_N = I$ by the law of large numbers.

** metropolis-hastings
  https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm

  get sequence of samples from some probability distribution.
  all you need is some function f propto the distribution.

  generate potential move; with prob f(proposal)/f(current), accept the move. otherwise, stay where you are.

  this turns out to work.


* Gen paper
  > Probability theory provides specifications for various tasks involving generative models including inferring the values of latent variables from values of observed variables given an assumed generative model. This task, which is called posterior inference in the context of Bayesian statistics, requires conditioning the joint distribution over latent and observed variables on the event that the observed variables took certain values. The latent variables may include numerical variables (often called parameters) as well as combinatorial discrete objects (often called structure). Inferences about the latent variables may take the form of (i) samples from the conditional probability distribution on the latent variables, (ii) an analytic representation of the conditional distribution, or (iii) some summary of the conditional distribution, such as the single value with maximum probability (as in maximum a posteriori or ‘MAP’ inference).

  monte carlo: list of samples from posterior (e.g. rejection sampling, MCMC)

  variational: get a distribution numerically optimized to closely match desired distribution

  key insight: metropolis-hastings in Gen is not fitting the "best" parameters. it's generating samples from the posterior distribution on the parameters, conditioned on the data.

* TODO From Scratch: Bayesian Inference, Markov Chain Monte Carlo and Metropolis Hastings, in python
  https://towardsdatascience.com/from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-in-python-ef21a29e25a
